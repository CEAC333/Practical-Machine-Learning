---
output: html_document
---
# Practical Machine Learning Data Project

### Background:
In this project, we are going to analyze the different data from many devices such as Jawbone Up, 
Nike Fuelband, and Fitbit. It is going to be a prediction of 20 different test cases. This data is based off of 6 different participants. The analysis is used to determine if the test cases are used correctly or incorrecly too.



## Loading All Datasets

In this section, all the packages are going to be loaded to run machine learning dataset.

```r
library(knitr)
library(caret)
library(rpart)
library(rattle)
library(randomForest)
library(corrplot)
library(gbm)
library(ipred)
library(xgboost)
```


## Reading All the Training and Testing Datasets

For this section, we are going to download the datasets and to get the train and test information out of there.

```r
set.seed(23425)
training <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
testing <- read.csv(url("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
inTrain <- createDataPartition(training$classe, p=0.75, list=FALSE)
TrainSet <- training[inTrain, ]
TestSet <- training[-inTrain, ]
```


## Elminiating NAV and NZV Values

Cleaning up datasets in order to be tidy.

```r
NAV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NAV]
TestSet <- TestSet[, -NAV]
```


## Removing the First Set of Variables


```r
TrainSet <- TrainSet[, -(1:10)]
TestSet <- TestSet[, -(1:10)]
```



## Unmeaningful Datasets to Remove


```r
NAP <- sapply(TrainSet,function(x)mean(is.na(x)))>0.95
TrainSet <- TrainSet[,NAP==FALSE]
TestSet <- TestSet[,NAP==FALSE]
```


## Random Forest Method


```r
set.seed(23223)
controlRF <- trainControl(method="cv", number=2, verboseIter=FALSE)
modFitRandForest <- train(classe ~ ., data=TrainSet, method="rf",trControl=controlRF)
predictRandForest <- predict(modFitRandForest, newdata=TestSet)
confMatRandForest <- confusionMatrix(predictRandForest, TestSet$classe)
modFitRandForest$finalModel
```

# Final Model of Random Forest Method


```r
Call:
randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 1.04%
Confusion matrix:
     A    B    C    D    E class.error
A 4180    4    0    1    0 0.001194743
B   30 2806   11    1    0 0.014747191
C    0   25 2540    2    0 0.010518115
D    0    0   71 2338    3 0.030679934
E    0    0    0    5 2701 0.001847746
```

## Generated Boosted Model


```r
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 4, repeats = 1)
modFitGBM <- train(class ~ ., data=TrainSet, method = "gbm",trControl = controlGBM, verbose = FALSE)
predictGBM <- predict(modFitGBM, newdata=TestSet)
confMatGBM <- confusionMatrix(predictGBM, TestSet$classe)
confMatGBM
```


## Confusion Matrix and Statistics


```r
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1373   55    4    7    5
         B   14  849   42   11   20
         C    4   34  796   48   24
         D    4    7   11  723   18
         E    0    4    2   15  834

Overall Statistics
                                          
               Accuracy : 0.9329          
                 95% CI : (0.9255, 0.9398)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.915           
 Mcnemar's Test P-Value : 6.064e-14       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9842   0.8946   0.9310   0.8993   0.9256
Specificity            0.9798   0.9780   0.9728   0.9902   0.9948
Pos Pred Value         0.9508   0.9071   0.8786   0.9476   0.9754
Neg Pred Value         0.9936   0.9748   0.9852   0.9804   0.9835
Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
Detection Rate         0.2800   0.1731   0.1623   0.1474   0.1701
Detection Prevalence   0.2945   0.1909   0.1847   0.1556   0.1743
Balanced Accuracy      0.9820   0.9363   0.9519   0.9447   0.9602
```



## Predictions for the 20 Test Data Sets


```r
predictionsRF <- predict(modFitRandForest, newdata=testing)
predictionsRF
```

```r
[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```


## Plots of Predictions of the Machine Learning Attempts


```r
plot(predictionsRF, col="blue",xlab="Prediction",ylab="Reference",main="20 Different Predictions")
```

![](PML1/20 Different Predictions.png)

# Conclusion:


Based on the information that is given from the different methods, it has been determined that
the Random Forest Method has a better accuracy. With the Random Forest Method, it is performed at 93.29% accuracy and estimated error rate of 1.04%.